{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06a389fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de50f875",
   "metadata": {},
   "source": [
    "# Data Generation\n",
    "\n",
    "No need to pay attention to this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8fb9260",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "noise_rate = 0.1\n",
    "ground_truth = np.random.uniform(0, 1, N) < noise_rate\n",
    "model_data_1 = np.array(ground_truth)\n",
    "mask = np.random.uniform(0, 1, N) < 0.2\n",
    "model_data_1[mask] = ~ground_truth[mask]\n",
    "#model_data_2 = np.random.uniform(0, 1, N) < noise_rate\n",
    "model_data_2 = np.zeros_like(ground_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712fc3fd",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "There are 2 machine learning models predicting whether or not if a patient has cancer. Model 1 generated `model_data_1` and Model 2 generated `model_data_2`. You are given `ground_truth` as the ground truth of whether or not if a patient has a cancer. Your task is to use the precision and recall of each model to decide which one is better. For the purposes of cancer detection, the worst case scenario for a patient is that they do have cancer but you miss the detection: a false positive is not as bad as a false negative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89802a2",
   "metadata": {},
   "source": [
    "# Problem 1\n",
    "Tabulate the confusion matrix for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6dd8398",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_confusion_matrix(ground_truth, model_data):\n",
    "    tp = (ground_truth & model_data).sum()\n",
    "    tn = (~ground_truth & ~model_data).sum()\n",
    "    fp = (~ground_truth & model_data).sum()\n",
    "    fn = (ground_truth & ~model_data).sum()\n",
    "    confusion_matrix = np.array([[tp, fp], [fn, tn]])\n",
    "    return confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f741c119",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_1 = create_confusion_matrix(ground_truth, model_data_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79f1aa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix_2 =create_confusion_matrix(ground_truth, model_data_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0500611f",
   "metadata": {},
   "source": [
    "# Problem 2 \n",
    "Calculate accuracy, precision and recall for each model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b12d61c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(confusion_matrix, N):\n",
    "    # tpr + tnr\n",
    "    return (confusion_matrix[0][0] + confusion_matrix[1][1])/N*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "950e7b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79.60000000000001\n",
      "91.2\n"
     ]
    }
   ],
   "source": [
    "print(calculate_accuracy(confusion_matrix_1, N))\n",
    "print(calculate_accuracy(confusion_matrix_2, N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31202009",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision(confusion_matrix):\n",
    "    # tp / (tp + fp)\n",
    "    return confusion_matrix[0][0]/(confusion_matrix[0][0] + confusion_matrix[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5ae51b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_recall(confusion_matrix):\n",
    "    # tp / (tp + fn)\n",
    "    return confusion_matrix[0][0]/(confusion_matrix[0][0] + confusion_matrix[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52fea644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27165354330708663\n",
      "nan\n",
      "0.7840909090909091\n",
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/q0/tyd7fw0s1pvb52jk2g63krdh0000gn/T/ipykernel_1292/367296156.py:3: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  return confusion_matrix[0][0]/(confusion_matrix[0][0] + confusion_matrix[0][1])\n"
     ]
    }
   ],
   "source": [
    "print(calculate_precision(confusion_matrix_1))\n",
    "print(calculate_precision(confusion_matrix_2))\n",
    "print(calculate_recall(confusion_matrix_1))\n",
    "print(calculate_recall(confusion_matrix_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074c2093",
   "metadata": {},
   "source": [
    "# Problem 3\n",
    "Describe which model performed better and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddbcdf6",
   "metadata": {},
   "source": [
    "When it comes to comparing two models, accuracy might not be the best metric. In our situation, the probability of a patient having a cancer is generally low. So, if you have a model that outputs False all the time like our `model_data_2`, it might have a higher accuracy but its precision is meaningless and its recall is 0. In cancer detection, false negatives are very bad. Recall measures what our model does when the patient really has cancer, while precision describes how well our model is performing when it says the patient has a cancer. So in cancer detection, recall is more meaningful than precision. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034f108b",
   "metadata": {},
   "source": [
    "# Reference\n",
    "https://www.nbshare.io/notebook/626706996/Learn-And-Code-Confusion-Matrix-With-Python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a0269f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
